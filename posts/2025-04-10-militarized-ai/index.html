<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="light dark" name=color-scheme><meta content="Essays and notes about all sorts of things" name=description><title>Military AI Is More Likely Than You Think</title><link href=/favicon-16.png rel=icon sizes=16x16 type=image/png><link href=/favicon-32.png rel=icon sizes=32x32 type=image/png><link href=/favicon-180.png rel=icon sizes=180x180 type=image/png><link href=/favicon-524.png rel=icon sizes=524x524 type=image/png><style>body{--primary-color:#5871a2;--primary-pale-color:#5871a233;--primary-decoration-color:#5871a210;--bg-color:#fff;--text-color:#2f3030;--text-pale-color:#767676;--text-decoration-color:#a9a9a9;--highlight-mark-color:#5f75b020;--callout-note-color:#5871a2;--callout-tip-color:#268556;--callout-important-color:#885fc9;--callout-warning-color:#ab6632;--callout-caution-color:#c64e4e}body.dark{--primary-color:#6f8fd1;--primary-pale-color:#6f8fd166;--primary-decoration-color:#6f8fd112;--bg-color:#1c1c1c;--text-color:#c1c1c1;--text-pale-color:#848484;--text-decoration-color:#5f5f5f;--highlight-mark-color:#8296cb3b;--callout-note-color:#6f8fd1;--callout-tip-color:#47976f;--callout-important-color:#9776cd;--callout-warning-color:#ad7a52;--callout-caution-color:#d06161}body{--main-font:ui-sans-serif,system-ui,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--code-font:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--homepage-max-width:768px;--main-max-width:768px;--avatar-size:56px;--font-size:16px;--line-height:1.75;--img-border-radius:0px;--detail-border-radius:0px;--dark-mode-img-brightness:.75;--dark-mode-chart-brightness:.75;--inline-code-border-radius:2px;--inline-code-bg-color:var(--primary-decoration-color);--block-code-border-radius:0px;--block-code-border-color:var(--primary-color);--detail-border-color:var(--primary-color)}</style><link href=https://blackpath.blog/main.css rel=stylesheet><link href=/hl-dark.css id=hl rel=stylesheet><body class="post dark"><div id=wrapper><div id=blank></div><aside><button aria-label="back to top" id=back-to-top><svg class="feather feather-arrow-up" viewbox="0 0 24 24" fill=none height=18 stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 width=18 xmlns=http://www.w3.org/2000/svg><line x1=12 x2=12 y1=19 y2=5></line><polyline points="5 12 12 5 19 12"></polyline></svg></button></aside><main><header><nav><a href=https://blackpath.blog/posts id=backlink>‚Üê Back</a></nav></header><div><div data-check-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18" height="18"><path d="M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z" fill="currentColor"></path></svg>
' data-copy-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18" height="18"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>
' id=copy-cfg style=display:none></div><article class=prose><h1 class=post-title><span>Military AI Is More Likely Than You Think</span> </h1><div id=post-info><div id=date><span id=publish>Apr 10, 2025</span></div><div id=tags><a class=instant href=https://blackpath.blog//tags/ai><span>#</span>AI</a><a class=instant href=https://blackpath.blog//tags/politics><span>#</span>Politics</a><a class=instant href=https://blackpath.blog//tags/military><span>#</span>Military</a></div></div><figure><img alt="CROWS weapon platform" src=/images/crows_platform.jpg><figcaption><small>Despite appearances, the CROWS weapon platform is not a friendly and lovable robot sidekick</small></figcaption></figure><p>If you haven't read <a rel="noopener nofollow noreferrer" href=https://ai-2027.com/ target=_blank>AI 2027</a>, you should. It's a story (model? scenario?) describing how superhuman AI could be created and what might happen if it is, and the authors include a huge amount of justification for their conclusions. It's plausible, although by the end it begins to creep into the ridiculous. It might be <em>actually</em> ridiculous, or I could just be falling into the <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Argument_from_incredulity target=_blank>classic trap</a> of believing that anything that seems weird to me personally can't happen. We'll find out around 2028 or 2030, I suppose.<p>I disagree strongly with one part of it though. To explain why I disagree, I'll need to tell a story about a panel on AI safety that I attended at <a rel="noopener nofollow noreferrer" href=https://fedsupernova.com/ target=_blank>FED Supernova</a> in 2024.<p>FED Supernova is a conference held every year in Austin, Texas. There are booths for companies that are selling unique technology to various parts of the military- everything from project management software to high tech bandages. In 2024, I was walking around the lobby when I saw a what looked like a Boston Dynamics robot with armor plating and rails (for mounting guns) attached to it.<p>I talked to the operator for a bit. He claimed they could be controlled remotely or run autonomously, were aware of other nearby units, and could move automatically in packs. They could also serve as a weapons platform for anything up to a <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/FGM-148_Javelin target=_blank>Javelin missile</a>, and could handle the recoil from a .50 caliber machine gun. Fun stuff.<figure><img alt="Tactical robot dog" src=/images/fed_supernove_2024.jpg><figcaption><small>Cool as hell, even if it ends up killing us all</small></figcaption></figure><p>Back in 2017 or so when I first became aware of <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Boston_Dynamics#Spot target=_blank>Spot</a>, the Boston Dynamics version of this robot, my first thought was "if someone isn't already working on turning that thing into a weapon, they will be shortly." Watching the weaponized version of that robot walk around the lobby during a DoD technology conference in 2024, my immediate thought was "if someone isn't already trying to automate tactical decision making for those things, they will be shortly." I'm not an expert on AI, but as far as I can tell, the technology already exists, although it might not have been assembled from the various available models and data ingestion tools.<p>You would need to describe a battlefield in a way that an AI could understand, have a model trained on a huge number of historical engagements, and convert the output of such a model into orders for packs of robot dogs outfitted with various complementary weapons systems.<p>All of which is possible now. Actually, all of this was possible in <em>2024</em>, so I'd guess it's probably being field tested somewhere now.<p>With these thoughts in my head, I went into the next panel that I was planning to attend- a discussion of AI ethics and safety. I'm, again, not an expert on either AI or the ethics and alignment issues surrounding the field, but I come in contact with people who are and sometimes read through discussion or blog posts on the topic. But even I could see that the panel wasn't engaging with any particularly difficult questions. When they asked for questions from the audiance, I raised my hand for the microphone.<blockquote><p>There's a robot dog in the lobby that's partially autonomous and capable of coordinating with other robot dogs. If the technology necessary to make autonomous tactical decisions for a fireteam of those robots doesn't currently exist, it will very shortly. It seems likely that, after the tactical level is automated, the strategic decision making will be as well.<p>In a confrontation between approximately equal military forces, one of which must report the current state of the battlefield up a chain of command, deliberate, and then pass orders back down the chain of command, and another which can make strategic and tactical decisions immediately, with complete knowledge of the current state of the battlefield, it seems obviously true that, all other things being equal, the military force that can act more quickly is likely to win.<p>Given this, there would be immense pressure on the US military to move toward heavy automation and remove humans from the loop in order to remain competitive. Is this a discussion that people in the military are having? Do they or you consider this to be a dangerous trend? And if so, are there any plans to mitigate this danger?</blockquote><p>The panelists all smiled, looked at each other, and shrugged. We sat in silence for a moment, and then the moderator asked if any of them would like to respond. After another brief silence, in which the panelists continued to smile vaguely out at the audiance, he answered me himself. His reply was bland, reassuring, and completely missed the point of the question.<p>As I walked away from that panel, annoyed and uneasy, it occurred to me that everyone on that panel likely had a security clearance and access to classified information about military AI efforts. Their lack of response, in a way, <em>was</em> a response.<p>Reading through the AI 2027 scenario, my first thought was that the authors are <em>heavily</em> underestimating how quickly AI will be adopted by the US military. These are not people who wonder if they are falling prey to the argument from incredulity, who speculate about the dangers of superhuman AI, or who give a shit about alignment. They will nod along with whatever the AI researchers say, and then ask what the earliest expected date for a field deployment might be. The following quote, in particular, is the most improbable thing I read in this story-<blockquote><p>The President is troubled. Like all politicians, he‚Äôs used to people sucking up to him only to betray him later. He‚Äôs worried now that the AIs could be doing something similar. Are we sure the AIs are entirely on our side? Is it completely safe to integrate them into military command-and-control networks? How does this ‚Äúalignment‚Äù thing work, anyway?</blockquote><p>Not only because the idea of Donald Trump soberly considering these questions is laughable, but because at this point in the story I would find it incredibly unlikely that they haven't <em>already</em> integrated these agents (or military variants of them) into our command-and-control networks. We have semi-autonomous drones in the air and on the ground. We have <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/CROWS target=_blank>weapons systems</a> that can be operated remotely and automatically track movement. We have a global network of intelligence, targeting, and surveillance systems producing huge amounts of data that <em>cannot</em> be analyzed by human beings at a useful speed.<p>Even Agent-1 in the AI 2027 scenario would be able to automate or partially automate many of these systems. By Agent-3-mini (the point where the president becomes troubled in the above quote), I would expect full tactical automation of most drones, mostly-automated targeting for most weapons platforms, and partially automated strategic planning.<p>I sincerely doubt that a future superintelligence would have to find some way to <em>convince</em> the US government to give it control over our military forces. It seems much more likely that it will be born with that control already in place.</article></div><footer><div class=left><div class=copyright>¬© 2025 Michael House</div></div><div class=right><a href=https://blackpath.blog/posts/feed.xml id=rss-btn>RSS</a></div></footer><dialog id=rss-mask><div><a href=https://blackpath.blog/posts/feed.xml>https://blackpath.blog/posts/feed.xml</a><button data-check-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18" height="18"><path d="M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z" fill="currentColor"></path></svg>
' data-copy-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18" height="18"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>
' aria-label=copy autofocus data-link=https://blackpath.blog/posts/feed.xml><svg viewbox="0 0 24 24" height=18 width=18 xmlns=http://www.w3.org/2000/svg><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill=currentColor></path></svg></button></div></dialog></main></div><script src=/js/lightense.min.js></script><script src=/js/popper.min.js></script><script src=/js/tippy.min.js></script><script>const referrer = document.referrer;
  const domain = window.location.origin;

  console.log(referrer);
  
  // if the visitor is coming from offsite, then remove
  // the lastUrl from local storage
  if(!!referrer && !referrer.includes(domain)){
    localStorage.removeItem('lastUrl');
  }

  const backButton = document.getElementById('backlink');
  const lastUrl = localStorage.getItem('lastUrl');

  // if a backlink has been set in local storage, then update
  // the actual button on the page to go there
  if(lastUrl != null) {
    backButton.href = lastUrl;
  }

  const links = document.getElementsByClassName('footnote-reference');
  const notes = document.getElementsByClassName('footnote-definition');

  for(let i = 0; i < links.length; ++i){
    let id = links[i].getElementsByTagName('a')[0].getAttribute('href').replace('#','');
    for(let j = 0; j < notes.length; ++j){
      if(id == notes[j].id){
        let content = notes[j].getElementsByTagName('p')[0];
        tippy(links[i], {
          content: content.cloneNode(true),
          interactive: true,
          allowHTML: true,
        });
      }
    }
  }</script><script src=https://blackpath.blog/js/main.js></script>